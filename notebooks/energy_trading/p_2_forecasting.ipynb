{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - Optimization Utilizing Forecasting\n",
    "Suppose you have to decide at the beginning of each day the times at which you are going to charge and discharge, without any knowledge of the future, i.e., any price information of that day or future days. You do have information about past prices, however, which you may use to predict buy and sell periods. Can you design a strategy for charging and discharging under these circumstances? For this question, go back to the battery assumptions of 1.1, i.e. a 1MW, 1MWh battery.\n",
    "\n",
    "Please explain your approach and in particular how you would train your trading strategy based on available information at any point in time. Illustrate your findings, and the difference to your results in question 1.1.\n",
    "\n",
    "\n",
    "\n",
    "## Solution\n",
    "\n",
    "One can use price forecast for the next day or hours to get better insight and support decision on when to buy or sell energy. Alternatively, based on the prediction (assuming we are confident with the results), we can then use the optimization functions to decide on the strategy to buy and sell. However, this strategy will only propagate what the forecast already tells us. Instead of using this two steps, one could model this problem as a classification, in which the model forecasts the action at each point in time. Effectivelly, forecast buy or sell. Obviously, this is a complex task since the model also need to understand the other constraints in which we can operate (simple example: is the battery empty or full?).\n",
    "\n",
    "In any case, for this particular take home exercice, I will stick with forecasting price.\n",
    "\n",
    "We want to predict `t+1` value based on `N` previous data points. For example, having close hourly prices from past days or even weeks, we want to predict what price will be on the next hour(s). Our Forecast Horizon will be the full next day.\n",
    "\n",
    "I will train a set of models with different algorithm varing complexity to check which one would perform better in this experimental setting. \n",
    "\n",
    "I've stumbled upon a few recent articles proposing methods specificaly tailored for energy price forecasting, which alludes to the fact they are much better for this problem than the basic models I'm choosing for this exercise. \n",
    "The principles remains the same, but obviously with the caveats and intricacies of the specifics of each model.\n",
    "\n",
    "As you will see at the end of this notebook, the basic model performs very poorly.\n",
    "\n",
    "In scope:\n",
    "* Linear regression, for simplicity\n",
    "* Random Forest regressor\n",
    "* Gradient Boosting\n",
    "* ARIMA\n",
    "* SARIMA\n",
    "* TBATS - Trigonometric seasonality, Box-Cox transformation, ARMA errors, Trend and Seasonal components\n",
    "* NeuralProphet\n",
    "\n",
    "**Model Evaluation Metrics:**\n",
    "* Mean Absolute Error (MAE): Measures the average magnitude of the errors without considering their direction. It's the simplest and most intuitive performance metric.\n",
    "* Root Mean Squared Error (RMSE): Gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Data preparation and data partition (train_data and test_data).\n",
    "2. Model training and validation. Due to time and resources constraints, I will not perform cross-validation, but rather only train-test cycle. In production scenario I would consider cross-validation.\n",
    "3. Forecast (with the 'best' performing model) ahead by a certain period of time for which you want to predict.\n",
    "\n",
    "**TODOs**\n",
    "* hyperparameter tuning and optimization\n",
    "* implement the missing models\n",
    "\n",
    "\n",
    "### References\n",
    "- [An Intra-Day Electricity Price Forecasting Based on a Probabilistic Transformer Neural Network Architecture](https://www.researchgate.net/publication/374091409_An_Intra-Day_Electricity_Price_Forecasting_Based_on_a_Probabilistic_Transformer_Neural_Network_Architecture)\n",
    "- [Short-Term Electricity Prices Forecasting Using Functional Time Series Analysis](https://www.mdpi.com/1996-1073/15/9/3423)\n",
    "- [Forecasting day-ahead electricity prices: Utilizing hourly prices](https://www.sciencedirect.com/science/article/abs/pii/S0140988315001668)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from pathlib import PurePath\n",
    "from environs import Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add custom python modules root to the path variable, so that we can resuse code\n",
    "root_path = PurePath(os.getcwd()).parents[1].joinpath(\"src\")\n",
    "if str(root_path) not in sys.path:\n",
    "    sys.path.insert(0, str(root_path))\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "env = Env()\n",
    "env.read_env(\".env\", override=True)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logging.getLogger(\"matplotlib\").setLevel(\n",
    "    logging.ERROR\n",
    ")  # Only show errors for this EDA since matplotlib is too verbose. Don't do this in production ;)\n",
    "\n",
    "dataset_path = env(\"ENERGY_TRADING_DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from tbats import BATS, TBATS\n",
    "from scipy import signal\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "from challenges.energy_trading.data_access import load_energy_data\n",
    "from typing import List, Tuple\n",
    "\n",
    "myseed = 31\n",
    "np.random.seed(myseed)\n",
    "random.seed(myseed)\n",
    "\n",
    "plt.rcParams.update({\"figure.figsize\": (10, 7), \"figure.dpi\": 200})\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation and Data partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_energy_data(filepath: str, suffix: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(filepath + suffix, skiprows=1, names=[\"DateTime\", \"Price\", \"Currency\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def propress_data(\n",
    "    df,\n",
    "    column: str,\n",
    "    periods: int,\n",
    "    date_fmt: str = \"%d.%m.%Y %H:%M\",\n",
    "    decompose_model: str = \"additive\",\n",
    "    decompose: bool = False,\n",
    "):\n",
    "    # Split the 'DateTime' column into 'start_time' and 'end_time'\n",
    "    datetime_splits = df[\"DateTime\"].str.split(\" - \", expand=True)\n",
    "    df[\"start_time\"] = pd.to_datetime(datetime_splits[0], format=date_fmt)\n",
    "    df[\"end_time\"] = pd.to_datetime(datetime_splits[1], format=date_fmt)\n",
    "\n",
    "    df = imput_missing_data(df, column)\n",
    "    df = aggregate_timewindows(df)\n",
    "    if decompose:\n",
    "        offset = abs(df[column].min()) + 0.01  # offseting by the smallest value + 1 cent of EUR\n",
    "        decomp = seasonal_decompose(df[column], period=periods, model=decompose_model, extrapolate_trend=\"freq\")\n",
    "        df[f\"{column}_offset\"] = df[column] + offset\n",
    "        df[f\"{column}_trend\"] = decomp.trend\n",
    "        df[f\"{column}_seasonal\"] = decomp.seasonal\n",
    "        df[f\"{column}_detrended\"] = df[column] - decomp.trend\n",
    "        df[f\"{column}_detrended_lsf\"] = signal.detrend(df[column])\n",
    "        df[f\"{column}_deseasonalized\"] = df[column] - decomp.seasonal\n",
    "        # differencing\n",
    "        df[f\"{column}_diff\"] = df[column].diff().dropna()\n",
    "        # log-transformed price, since we have no zeros in the Price column\n",
    "        df[f\"{column}_log\"] = np.log(df[column] + offset)  # handling zeros: use a small offset constant\n",
    "        # Seasonal differencing\n",
    "        df[f\"{column}_seasonal_diff\"] = df[column].diff(\n",
    "            periods\n",
    "        )  # assuming 24 observations per day for daily seasonality\n",
    "        # Combining Transformations:\n",
    "        df[f\"{column}_log_diff\"] = np.log(df[column] + offset).diff().dropna()\n",
    "\n",
    "    # clean up\n",
    "    df = df.dropna()\n",
    "    df = df.drop(columns=[\"DateTime\", \"end_time\", \"Currency\"])\n",
    "    df.set_index(\"start_time\", inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def aggregate_timewindows(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = data.copy()\n",
    "    df[\"month\"] = df.start_time.dt.month\n",
    "    df[\"day_of_month\"] = df.start_time.dt.day\n",
    "    df[\"week\"] = df.start_time.dt.isocalendar().week\n",
    "    df[\"day_of_week\"] = df.start_time.dt.isocalendar().day\n",
    "\n",
    "    # Generate 'business hour' feature\n",
    "    business_hours = ((df.start_time.dt.hour >= 7) & (df.start_time.dt.hour <= 12)) | (\n",
    "        (df.start_time.dt.hour >= 14) & (df.start_time.dt.hour <= 19)\n",
    "    )\n",
    "    lunch_hours = (df.start_time.dt.hour > 12) & (df.start_time.dt.hour < 14)\n",
    "\n",
    "    df[\"business_hours\"] = 0  # Default to off hour\n",
    "    df.loc[business_hours, \"business_hours\"] = 2  # Set to business hour\n",
    "    df.loc[lunch_hours, \"business_hours\"] = 1  # Set to lunch hour\n",
    "\n",
    "    # Generate 'weekend' feature\n",
    "    df[\"weekend\"] = 0  # Default to weekday\n",
    "    df.loc[df.day_of_week == 5, \"weekend\"] = 1  # Set to Saturday\n",
    "    df.loc[df.day_of_week == 6, \"weekend\"] = 2  # Set to Sunday\n",
    "    return df\n",
    "\n",
    "\n",
    "def imput_missing_data(df: pd.DataFrame, target_column: str) -> pd.DataFrame:\n",
    "    # Check for any conversion issues\n",
    "    if df[target_column].isnull().any():\n",
    "        logging.warning(f\"There are null values in the {target_column} column. Applying ffill.\")\n",
    "        df[target_column] = df[target_column].ffill()\n",
    "\n",
    "        if pd.isna(df[target_column].iloc[0]):\n",
    "            # Ensure no NaN values remain, if first value is NaN,\n",
    "            # replace it with a sensible default (e.g., 0 or mean of the column)\n",
    "            logging.warning(\"Imputing remaining missing values with median as fallback strategy.\")\n",
    "            df[target_column].iloc[0] = df[target_column].median()\n",
    "    if df[\"Currency\"].isnull().any():\n",
    "        df[\"Currency\"] = df[\"Currency\"].fillna(\"EUR\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_daily_forecast_splits(\n",
    "    df, test_size_percentage, n_splits, periods: int = 24\n",
    ") -> List[Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    test_days = int((len(df) / periods) * test_size_percentage)\n",
    "    test_size = test_days * periods\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits, test_size=test_size)\n",
    "\n",
    "    splits = []\n",
    "    for train_index, test_index in tscv.split(df):\n",
    "        if test_index[0] % periods != 0:\n",
    "            start_offset = periods - (test_index[0] % periods)\n",
    "            test_index = range(test_index[0] + start_offset, test_index[-1] + 1)\n",
    "        splits.append((df.iloc[train_index], df.iloc[test_index]))\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(models, splits):\n",
    "    results = {}\n",
    "    for model_name, model in models.items():\n",
    "        maes = []\n",
    "        rmses = []\n",
    "        for X_train, X_test in splits:\n",
    "            y_train, y_test = X_train[\"Price\"], X_test[\"Price\"]\n",
    "            X_train, X_test = X_train.drop(\"Price\", axis=1), X_test.drop(\"Price\", axis=1)\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            predictions = model.predict(X_test)\n",
    "\n",
    "            maes.append(mean_absolute_error(y_test, predictions))\n",
    "            rmses.append(np.sqrt(mean_squared_error(y_test, predictions)))\n",
    "\n",
    "        results[model_name] = {\"MAE\": maes, \"RMSE\": rmses}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to ensure there is not data leak in the training.\n",
    "# Since we have created some features based on the target feature, we should remove those, which are:\n",
    "# Price_offset, Price_trend, Price_seasonal, Price_detrended, Price_detrended_lsf, Price_deseasonalized, Price_diff, Price_log, Price_seasonal_diff, Price_log_diff\n",
    "# These columns were useful during the EDA phase. For the training step we set decompose=False, so that these columsn are not included\n",
    "e_prices_60m_df = propress_data(\n",
    "    load_energy_data(dataset_path, \"_60min.csv\"), column=\"Price\", periods=24, decompose=False\n",
    ")\n",
    "\n",
    "target_col = \"Price\"\n",
    "features_cols = [\"month\", \"day_of_month\", \"week\", \"day_of_week\", \"business_hours\", \"weekend\"]\n",
    "\n",
    "# Cross-validation\n",
    "data_splits = prepare_daily_forecast_splits(\n",
    "    e_prices_60m_df[features_cols + [target_col]], n_splits=5, test_size_percentage=0.1, periods=24\n",
    ")\n",
    "data_splits[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100),\n",
    "}\n",
    "\n",
    "results = train_and_evaluate(models, data_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, metrics in results.items():\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    for metric_name, values in metrics.items():\n",
    "        print(f\"\\t{metric_name}: avg {np.mean(values):.2f}, std: {np.std(values):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets assume the `RandomForestRegressor` performed better than the others. \n",
    "\n",
    "At the final stage of model training, especially after selecting a model based on cross-validation results, it's common practice to train the model on the most comprehensive dataset available to maximize its learning and performance.\n",
    "\n",
    "Thus, let's retrain the model and visualize the forecasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_data = e_prices_60m_df[features_cols + [target_col]]\n",
    "\n",
    "\n",
    "# Define features and target\n",
    "X = e_prices_60m_df[features_cols]\n",
    "y = e_prices_60m_df[target_col]\n",
    "\n",
    "\n",
    "# Splitting the data for final visualization (optional)\n",
    "split_point = int(len(X) * 0.9)  # For example, hold out the last 10% of data\n",
    "X_train, X_test = X.iloc[:split_point], X.iloc[split_point:]\n",
    "y_train, y_test = y.iloc[:split_point], y.iloc[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Gradient Boosting Regressor on all available data\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X_train, y_train)  # Assuming X_train and y_train include all data you plan to train on\n",
    "\n",
    "# If you want to visualize how the model performs on this data:\n",
    "predictions = model.predict(X_test)  # or on a new test set if available\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
    "\n",
    "# Plotting the forecasts against the actual outcomes\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test.index, y_test, label=\"Actual Prices\", color=\"blue\", marker=\"o\", linestyle=\"-\")\n",
    "plt.plot(y_test.index, predictions, label=\"Predicted Prices\", color=\"red\", marker=\"x\", linestyle=\"--\")\n",
    "plt.title(\"Comparison of Actual and Predicted Prices\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price (EUR/MWh)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

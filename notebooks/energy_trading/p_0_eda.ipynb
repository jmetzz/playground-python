{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Data Exploration\n",
    "\n",
    "Task definition:\n",
    "> Please summarize and compare both data sets statistically. Are there any patterns in the data that both share and are there others that differ between them? Can you make a recommendation for which type of contract would be more beneficial to trade in for Flexa?\n",
    "\n",
    "With a bigger and more complex dataset I would probably start the exploration using a tool like `ydata-profiling` to get a quick overview of the data distribution and features. For the dataset we have here with effectivelly only one feature of interest, I will not use such tool and try to analyse using basic `pandas` and/or some simple statistical models.\n",
    "\n",
    "As yo follow along the cells in this notebook, you will find my comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from pathlib import PurePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add custom python modules root to the path variable, so that we can resuse code\n",
    "root_path = PurePath(os.getcwd()).parents[1].joinpath(\"src\")\n",
    "if str(root_path) not in sys.path:\n",
    "    sys.path.insert(0, str(root_path))\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logging.getLogger(\"matplotlib\").setLevel(\n",
    "    logging.ERROR\n",
    ")  # Only show errors for this EDA since matplotlib is too verbose. Don't do this in production ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environs import Env\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from challenges.energy_trading.data_access import load_energy_data\n",
    "\n",
    "myseed = 31\n",
    "np.random.seed(myseed)\n",
    "random.seed(myseed)\n",
    "\n",
    "plt.rcParams.update({\"figure.figsize\": (20, 4), \"figure.dpi\": 200})\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env()\n",
    "env.read_env(\".env\", override=True)\n",
    "\n",
    "dataset_path = env(\"ENERGY_TRADING_DATA\")\n",
    "dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and prep\n",
    "\n",
    "\n",
    "The 2 csv files we will load have a similar scheme. The first row represents the header and the following rows the entries for each respective periodicity, either hourly or every 15 minutes.\n",
    "\n",
    "`\"MTU (CET/CEST)\",\"Day-ahead Price [EUR/MWh]\",\"Currency\",\"BZN|DE-LU\"\n",
    "\"01.01.2022 00:00 - 01.01.2022 00:15\",\"69.17\",\"EUR\"`\n",
    "\n",
    "The last column named `BZN|DE-LU` is missing for all records in both files. I believe this column should contain information related to bidding zones in Germany and Luxembourg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_energy_data(filepath: str, suffix: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(filepath + suffix, skiprows=1, names=[\"DateTime\", \"Price\", \"Currency\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "e_prices_raw_60m = load_energy_data(dataset_path, \"_60min.csv\")\n",
    "e_prices_raw_15m = load_energy_data(dataset_path, \"_15min.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_prices_raw_60m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_prices_raw_15m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both data sets are loaded but not cleaned/processed just yet.\n",
    "Let's start with the data exploration to get a better understanding of the data we have at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_prices_raw_60m.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_prices_raw_15m.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing to note is the column `DateTime` is not of date type, but rather a string (dtype `obj`). In order to simplify analysis, we will transform this column into two other columns that represent the time limits for this price: `start_time` and `end_time`.\n",
    "\n",
    "We can also see some missing values for the `Price` and `Currency` columns in both data sets. Let's double-check this aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> 60min data:\")\n",
    "print(e_prices_raw_60m[e_prices_raw_60m.isnull().any(axis=1)])\n",
    "print(\"-\" * 90)\n",
    "print(\"\\n>>> 15min data:\")\n",
    "print(e_prices_raw_15m[e_prices_raw_15m.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the entry `2042` correspoing to the timestamp `27.03.2022 02:00 - 27.03.2022 03:00` is missing the price and currency information in the `60min` data, and there are 4 records with missing data in the `15min` data set.\n",
    "\n",
    "We should also check if all prices are specified in the same currency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> 60min data:\")\n",
    "print(e_prices_raw_60m[\"Currency\"].unique())\n",
    "print(\"-\" * 90)\n",
    "print(\"\\n>>> 15min data:\")\n",
    "print(e_prices_raw_15m[\"Currency\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, all records have either \"EUR\" or nan in the `Currency` column. Since our data represents information from bidding zone `BZN|DE-LU` and as stated in the data header, it is safe to assume `EUR` as the currency for all records and use it to handle the missing data in the `Currency` column.\n",
    "\n",
    "We can handle missing values in several ways (called Imputation), which is totally dependent on the problem domain. Examples:\n",
    "* Removing entries with missing values\n",
    "* Forward fill or back fill: for time series data (like our prices data), it's common to use methods like forward filling (ffill) or backward filling (bfill) to fill in missing values based on nearby data points.\n",
    "* Using statistical mthods: we could also fill missing values with the mean, median, or mode of the column, which might be appropriate depending on the distribution and the nature of the data. There are other more powerful algorithms to be considered too.\n",
    "* Interpolation: estimates values using existing data points. Linear interpolation is common, but other methods (like polynomial or spline interpolation) can be more suitable depending on the nature of the time series.\n",
    "* Regression Models: can estimate missing values.\n",
    "\n",
    "Here is a **simplistic example** of a `RandomForestClassifier` to predict missing values:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Split the data into sets with and without missing values\n",
    "train_df = df[df['Price'].notna()]\n",
    "test_df = df[df['Price'].isna()]\n",
    "\n",
    "# Training a classifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(train_df.drop('Price', axis=1), train_df['Price'])\n",
    "\n",
    "# Predicting the missing values\n",
    "predicted_values = clf.predict(test_df.drop('Price', axis=1))\n",
    "df.loc[df['Price'].isna(), 'Price'] = predicted_values\n",
    "```\n",
    "\n",
    "There are more sofisticated strategies, like the 'Multiple Imputation' approach where each missing value is imputed multiple times to generate a distribution of possible values, thus, better modeling/capturing the uncertainty around the true value. The `fancyimpute` python package provides some options to implement this strategy. \n",
    "\n",
    "\n",
    "For simplicity, in this example we will use `EUR` for missing data in the `Currency` column, and a combination of backward filling and median for the `Price` column. The idea here is the fall back to `median` in case the `bfill` fails.\n",
    "\n",
    "But first, let's look at some descriptive statistics to get some insights on the data distribution and central tendencies of the data, and see if there are any obvious outliers, anomalies, or interesting patterns. The median, mean, standard deviation, minimum, maximum, and some percentiles of the energy prices can be valuable.  Then we can apply the imputation strategy and see how ti changes the data distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a small set of statistic metrics to summarize the dataset\n",
    "\n",
    "print(\">>> 60min data:\")\n",
    "print(e_prices_raw_60m.describe())\n",
    "print(\"-\" * 90)\n",
    "print(\"\\n>>> 15min data:\")\n",
    "print(e_prices_raw_15m.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What can we see in the `60min` data?\n",
    "* The mean (average) price is approximately €185.80/MWh. This value gives you a central point of the data's distribution, suggesting that on average, the price per megawatt-hour across all observations is around this value.\n",
    "* The standard deviation is about €90.66/MWh, indicating a high level of volatility in the price data. A high standard deviation in energy prices is common due to various factors affecting the market, such as changes in demand, fuel costs, weather conditions, and regulatory changes. The large value here tells you that the prices frequently vary a significant amount from the mean, highlighting the market’s unpredictability, which imposes an important challenge to predict a robust buy-sell strategy.\n",
    "* The minimum price is -€19.04/MWh, which is quite unusual as it represents a **negative** price. I can imagine this could be a valid scenarion when there is an excess supply (e.g., high renewable generation) and not enough demand, making the producers to pay consumers to take the excess energy off the grid. This is also a good opportunity for consuming energy (buy it) to sell later at much higher price and thus, increaee profitability.\n",
    "* The percentiles:\n",
    "  * 25th (or first quartile) indicates that 25% of the prices are below €122.15/MWh. So only a quarter of the time on average we could benefit from much lower than average price.\n",
    "  * 50% (Median) is €189.00/MWh, tells us that half the prices are above this value, and half are below. Thus, in 50% of the time we could sell above average price.\n",
    "  * 75th (or third quartile) shows that 75% of the prices are below €232.94/MWh. Conversely, about 25% of the prices are higher than this value, which gives us 25% chance to sell at very good prices to increase profit.\n",
    "* The maximum price observed is €700.00/MWh, highlighting the upper extreme of the market prices. This could be due to extraordinary demand or limited supply conditions (global crisis, extreme weather, or even local politics can affect the energy supply chain), and it marks scenarios where selling energy could be highly profitable.\n",
    "\n",
    "\n",
    "What can we see in the `15min` data?\n",
    "* the mean is very close to the mean and the percentiles in the `60min` data with just a small difference\n",
    "* the variation in the `15min` data is much more pronounced, with standard deviation `13.49%` higher in the `15min` dataset.\n",
    "* peaks and valeis are also more pronounced in ten `15min` data, repectively `max = 2999.99` and `min = -149.99`, which if exploited properly represent much better opportunities for increased profit.\n",
    "\n",
    "\n",
    "**In summary**, the wide price variation and the high standard deviation suggest a highly volatile market with significant price fluctuations, which could provide opportunities for profit through timely buying and selling strategy. For now, due the the nature of high resolution and locality, the `15min` data presents slightly more oppotunities for profit. We still need to double-check this assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imput_missing_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Check for any conversion issues\n",
    "    if df[\"Price\"].isnull().any():\n",
    "        logging.warning(\"There are null values in the Price column\")\n",
    "        logging.warning(\"Imputing missing values with ffill.\")\n",
    "        df[\"Price\"] = df[\"Price\"].ffill()\n",
    "\n",
    "        if pd.isna(df[\"Price\"].iloc[0]):\n",
    "            # Ensure no NaN values remain, if first value is NaN,\n",
    "            # replace it with a sensible default (e.g., 0 or mean of the column)\n",
    "            logging.warning(\"Imputing remaining missing values with median as fallback strategy.\")\n",
    "            df[\"Price\"].iloc[0] = df[\"Price\"].median()\n",
    "    if df[\"Currency\"].isnull().any():\n",
    "        df[\"Currency\"] = df[\"Currency\"].fillna(\"EUR\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_prices_60m = imput_missing_data(e_prices_raw_60m)\n",
    "e_prices_15m = imput_missing_data(e_prices_raw_15m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check if all the missing data issues were solved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> 60min data:\")\n",
    "print(e_prices_60m.info())\n",
    "print(\"-\" * 90)\n",
    "print(\"\\n>>> 15min data:\")\n",
    "print(e_prices_15m.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values are now handled in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> 60min data:\")\n",
    "print(e_prices_60m.describe())\n",
    "print(\"-\" * 90)\n",
    "print(\"\\n>>> 15min data:\")\n",
    "print(e_prices_15m.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected there is not much change in the data distribution. We only had a handful of data points with missing data in a much bigger number of records, and thus, the imputation does not change the distribution significantly, which is exactly what we want.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit the `DateTime` column. We need to split it into two other columns that represent the time limits for this price: `start_time` and `end_time`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame, date_fmt: str = \"%d.%m.%Y %H:%M\") -> pd.DataFrame:\n",
    "    \"\"\"Split the 'DateTime' column into 'start_time' and 'end_time'\"\"\"\n",
    "    datetime_splits = df[\"DateTime\"].str.split(\" - \", expand=True)\n",
    "    df[\"start_time\"] = pd.to_datetime(datetime_splits[0], format=date_fmt)\n",
    "    df[\"end_time\"] = pd.to_datetime(datetime_splits[1], format=date_fmt)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "e_prices_60m = preprocess_data(e_prices_60m)\n",
    "e_prices_15m = preprocess_data(e_prices_15m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> 60min data:\")\n",
    "print(e_prices_60m.head(5))\n",
    "print(\"-\" * 90)\n",
    "print(\"\\n>>> 15min data:\")\n",
    "print(e_prices_15m.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Double-check if we did not add any null values and verify the dtypes are correctly set:\\n\")\n",
    "\n",
    "print(\">>> 60min data:\")\n",
    "print(e_prices_60m.info())\n",
    "print(\"-\" * 90)\n",
    "print(\"\\n>>> 15min data:\")\n",
    "print(e_prices_15m.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify we do have the expected number of data point in each dataset.\n",
    "\n",
    "* `60min` data - since the dataset is covering 6 months (from January to June), we should expect to have `24 * number of days` data points. \n",
    "* `15min` data - in this case, we have 4 data points per hour, and thus, we expect `4 * 24 * number of days` data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of days between the two dates, inclusive of both start and end dates\n",
    "start_date = datetime(2022, 1, 1)\n",
    "end_date = datetime(2022, 6, 30)\n",
    "days_inclusive = (end_date - start_date).days + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60min data\n",
    "expected_data_points = days_inclusive * 24\n",
    "first_timestamp = e_prices_60m[\"start_time\"].min()\n",
    "last_timestamp = e_prices_60m[\"start_time\"].max()\n",
    "assert e_prices_60m.shape[0] == expected_data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15min data\n",
    "expected_data_points = days_inclusive * 24 * 4\n",
    "first_timestamp = e_prices_15m[\"start_time\"].min()\n",
    "last_timestamp = e_prices_15m[\"start_time\"].max()\n",
    "assert e_prices_15m.shape[0] == expected_data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> 60min data:\")\n",
    "print(f\"expected: {days_inclusive * 24} rows, got: {e_prices_60m.shape}\")\n",
    "print(\"-\" * 90)\n",
    "print(\"\\n>>> 15min data:\")\n",
    "print(f\"expected: {days_inclusive * 24 * 4} rows, got: {e_prices_15m.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions to generate the plots\n",
    "\n",
    "\n",
    "def plot_df(x: pd.Series, y: pd.Series, title: str = \"\", xlabel: str = \"Date\", ylabel: str = \"Value\"):\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.fill_between(x, y1=y, alpha=0.5, linewidth=2, color=\"seagreen\")\n",
    "    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_mirrored(\n",
    "    x: pd.Series,\n",
    "    y: pd.Series,\n",
    "    title: str = \"\",\n",
    "    series_label: str = \"\",\n",
    "    xlabel: str = \"Date\",\n",
    "    ylabel: str = \"Value\",\n",
    "    y_scale=800,\n",
    "):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 4))\n",
    "    plt.fill_between(x, y1=y, y2=-y, alpha=0.5, linewidth=2, color=\"seagreen\", label=series_label)\n",
    "    plt.ylim(-y_scale, y_scale)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.gca().set(xlabel=xlabel, ylabel=ylabel)\n",
    "    plt.hlines(y=0, xmin=np.min(x), xmax=np.max(x), linewidth=0.5)\n",
    "    # plt.hlines(y=0, xmin=np.min(df[x_col_name]), xmax=np.max(df[x_col_name]), linewidth=.5)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_positive_negative(x: pd.Series, y: pd.Series, title: str = \"\", xlabel: str = \"Date\", ylabel: str = \"Value\"):\n",
    "    plt.figure(figsize=(16, 4))\n",
    "\n",
    "    # Plot positive values in one color and negative values in another\n",
    "    plt.fill_between(x, y1=y, where=(y >= 0), alpha=0.5, linewidth=2, color=\"seagreen\", label=\"Positive\")\n",
    "    plt.fill_between(x, y1=y, where=(y < 0), alpha=0.5, linewidth=2, color=\"salmon\", label=\"Negative\")\n",
    "\n",
    "    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_boxplot_series(\n",
    "    df: pd.DataFrame, x_col_name: str, y_col_name: str, title: str = \"Title\", x_label: str = \"\", y_label: str = \"\"\n",
    "):\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    ax = sns.boxplot(x=x_col_name, y=y_col_name, data=df)\n",
    "\n",
    "    # Set titles and labels\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.set_xlabel(x_label, fontsize=14)\n",
    "    ax.set_ylabel(y_label, fontsize=14)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_n_boxplot_series(df, target_columns: List[str], title: str, y_label: str, fig_size: Tuple = (32, 4)):\n",
    "    # Number of target columns determines the number of subplots\n",
    "    num_plots = len(target_columns)\n",
    "\n",
    "    # Create a figure and a grid of subplots\n",
    "    fig, axes = plt.subplots(nrows=num_plots, ncols=1, figsize=(fig_size[0], fig_size[1] * num_plots))\n",
    "\n",
    "    # Flatten axes array if there's more than one subplot\n",
    "    if num_plots > 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]  # Ensure axes is iterable for a single subplot scenario\n",
    "\n",
    "    # Plot each column on a separate subplot\n",
    "    for i, column in enumerate(target_columns):\n",
    "        sns.boxplot(x=column, y=\"Price\", data=df, ax=axes[i], linewidth=2, color=\"skyblue\")\n",
    "        axes[i].set_title(f\"{title} - {column}\", fontsize=14)\n",
    "        axes[i].set_xlabel(column)\n",
    "        axes[i].set_ylabel(y_label)\n",
    "        axes[i].legend(loc=\"best\")\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_daily_aggregated_prices(df, title_prefix: str = \"\"):\n",
    "    plt.figure(figsize=(16, 4))\n",
    "\n",
    "    # Mean price line\n",
    "    sns.lineplot(x=\"hour_of_day\", y=\"mean\", data=df, marker=\"o\", label=\"Average Price\")\n",
    "\n",
    "    # Shaded area for standard deviation\n",
    "    plt.fill_between(df[\"hour_of_day\"], df[\"mean\"] - df[\"std\"], df[\"mean\"] + df[\"std\"], color=\"seagreen\", alpha=0.3)\n",
    "\n",
    "    plt.title(f\"{title_prefix}Average Energy Prices by Hour of Day\")\n",
    "    plt.xlabel(\"Hour of Day\")\n",
    "    plt.ylabel(\"Average Price (EUR/MWh)\")\n",
    "    plt.xticks(range(0, 24))  # Ensure all hours are shown\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_positive_negative(\n",
    "    x=e_prices_60m.start_time, y=e_prices_60m.Price, title=\"60min - Openning price\", ylabel=\"Price [EUR/MWh]\"\n",
    ")\n",
    "plot_positive_negative(\n",
    "    x=e_prices_15m.start_time, y=e_prices_15m.Price, title=\"15min - Openning price\", ylabel=\"Price [EUR/MWh]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `60min` data we can see a sharp increase in prices in mid February 2022 and peaked at around the mid March. That peak, which initially looked like a outlier clearly is not. It is part of the upward trend or energy prices increase that happened due to global economical situation.\n",
    "The prices went down after that peak and returned to the normal variation until around mid June, 2022.\n",
    "\n",
    "Also interesting to see the ~5 negative price occurencies, ploted as red lines.\n",
    "At first sight, it doesn't look like there is any seasonality or strong trend. I looks rather random series with a high volatility as expected in pricing domain.\n",
    "\n",
    "For the `15min` data, the situation is quite similar, but with more pronounced peaks and valeis. There seems to have a small number of extreme values around March, which could be a considered as outliers. However, givene the global events and crisis, this value could also be a legitimate value. I would digg in to external sources to validate this hypothesis and remove/replace these datapoints if they are indeed outliers.\n",
    "\n",
    "Let's explore it deeper, since at this scale is difficult to see any seasonality.\n",
    "I will check different level of aggregation windows, such as month, week, and day. For this we need to add some aggregated features/columns to the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_timewindows(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = data.copy()\n",
    "    df[\"month\"] = df.start_time.dt.month\n",
    "    df[\"day_of_month\"] = df.start_time.dt.day\n",
    "    df[\"week\"] = df.start_time.dt.isocalendar().week\n",
    "    df[\"day_of_week\"] = df.start_time.dt.isocalendar().day\n",
    "    # We could also filter out data if necessary to better understand specific periods.\n",
    "    # Example:\n",
    "    # expanded_df = expanded_df[(expanded_df['month'].isin([1, 2, 3])) & (expanded_df['week'] < 53)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_60min_df = aggregate_timewindows(e_prices_60m)\n",
    "expanded_60min_df[\"week\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_15min_df = aggregate_timewindows(e_prices_15m)\n",
    "expanded_15min_df[\"week\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> 60min data:\")\n",
    "print(expanded_60min_df.Price.isna().any())\n",
    "print(\"-\" * 90)\n",
    "print(\"\\n>>> 15min data:\")\n",
    "print(expanded_15min_df.Price.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that first week is in the list above is 52. This is because January 1st, 2022, was part of the last week of 2021 according to the ISO week date system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_boxplot_series(\n",
    "    expanded_60min_df,\n",
    "    [\"week\", \"month\", \"day_of_month\", \"day_of_week\"],\n",
    "    title=\"60min - Price Distribution Over 6 Months accross different periodicities\",\n",
    "    y_label=\"Price [EUR/MWh]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_boxplot_series(\n",
    "    expanded_15min_df,\n",
    "    [\"week\", \"month\", \"day_of_month\", \"day_of_week\"],\n",
    "    title=\"15min - Price Distribution Over 6 Months accross different periodicities\",\n",
    "    y_label=\"Price [EUR/MWh]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "These charts still don't show much patterns.\n",
    "Let's try to answer the following question:\n",
    "\n",
    "> Is there any correlation of prices with hour of the day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_60min_df[\"hour_of_day\"] = expanded_60min_df[\"start_time\"].dt.hour\n",
    "hourly_prices_60min_df = expanded_60min_df.groupby(\"hour_of_day\")[\"Price\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "\n",
    "expanded_15min_df[\"hour_of_day\"] = expanded_15min_df[\"start_time\"].dt.hour\n",
    "hourly_prices_15min_df = expanded_15min_df.groupby(\"hour_of_day\")[\"Price\"].agg([\"mean\", \"std\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_daily_aggregated_prices(hourly_prices_60min_df, title_prefix=\"60min - \")\n",
    "plot_daily_aggregated_prices(hourly_prices_15min_df, title_prefix=\"15min - \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start to see some patterns, in both datasets. Around 5 AM the prices start to raise, reaching peak around 7 and 8 AM, lowering until around 2 PM and starting a new cycle. This looks like a senoid with two peaks per day, at the 7th hour (7 AM and 7 PM).\n",
    "\n",
    "Thus, that is the moment to sell energy and realise the profit with respect to buys made around 3-4 hours earlier in the day, when prices are a lowes level.\n",
    "\n",
    "Let's also plot a full day of each dataset just for curiosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hourly_prices(day_data, title: str = \"\", periodicity: str = \"Hourly\", color=\"seagreen\"):\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.plot(day_data[\"start_time\"], day_data[\"Price\"], marker=\"o\", linestyle=\"-\", color=color, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(periodicity)\n",
    "    plt.ylabel(\"Price (EUR/MWh)\")\n",
    "    plt.xticks(day_data[\"start_time\"], day_data[\"start_time\"].dt.time, rotation=90)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "target_date = pd.to_datetime(\"2022-05-05\").date()\n",
    "plot_hourly_prices(expanded_60min_df[expanded_60min_df[\"start_time\"].dt.date == target_date], \"Hourly prices\")\n",
    "plot_hourly_prices(\n",
    "    expanded_15min_df[expanded_15min_df[\"start_time\"].dt.date == target_date], \"Quarter hour prices\", color=\"orange\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The price volatility within 1 hour is also very high (as expected for financial market). Which again brings more opportunities for making profit with a well chosen and timely buy-vs-sell action.\n",
    "\n",
    "I tend to believe this variation is even more pronounced in lower granularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposing the time series\n",
    "\n",
    "> NOTE: in this part I'm going to speedup a bit due to time constraints.\n",
    "\n",
    "Any time series can be seen as a combination of:\n",
    "- base level\n",
    "- trend: the longer-term movement in the data\n",
    "- seasonality: the refular pattern withing a fixed period\n",
    "- irreducible (residual) error: remainder of the data after trend and seasonal components have been removed\n",
    "\n",
    "There is also a difference between `additive` and `multiplicative` time series, which represent the nature of the seasonal effect and how to these components are combined. If the seasonal effect varies with the level of the time series (e.g., higher prices lead to proportionally larger seasonal swings), we should use multiplicative.\n",
    "\n",
    "**Additive**: `base` + `trend` + `seasonality` + $\\epsilon$\n",
    "\n",
    "**Multiplicative**: `base` * `trend` * `seasonality` * $\\epsilon$\n",
    "\n",
    "\n",
    "The `seasonal_decompose` in `statsmodels` implements the classical decomposition of a time series by considering the series as an additive or multiplicative combination of these components. However, multiplicative decomposition is not appropriate for zero and negative values, and therefore, we need to somehow change the negative prices we have in our timeseries if we want to use multiplicative decomposition.\n",
    "\n",
    "There are several strategies to deal with this transformation, for example:\n",
    "\n",
    "* Offsetting the Data: offset the entire dataset so that all values become positive. This is done by adding a constant to all data points in the series that is at least as large as the absolute value of the most negative number.\n",
    "* Log Transformation: If the data can logically scale logarithmically (which implies no zero values), applying a log transformation can also help manage negative values. This approach is useful particularly for multiplicative models where exponential growth or decay is expected.\n",
    "\n",
    "\n",
    "For simplicity, I will offset the data by a small constant shift which is just aa bit greater than the smallest negative number or zero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we ensure the dataframe has a datetime index, which is required for time series analysis using statsmodels.\n",
    "expanded_60min_df.set_index(\"start_time\", inplace=True)\n",
    "expanded_15min_df.set_index(\"start_time\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expanded_60min_df.index)\n",
    "print(expanded_15min_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets review the datasets\n",
    "expanded_60min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_15min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additive Decomposition\n",
    "# Here period defines the number of data points in each cycle.\n",
    "# For hourly data, we use 24 data points per day and 96 for 15-minute data (4 points per hour).\n",
    "result_add_60min = seasonal_decompose(expanded_60min_df[\"Price\"], model=\"additive\", period=24, extrapolate_trend=\"freq\")\n",
    "result_add_60min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_add_15min = seasonal_decompose(\n",
    "    expanded_15min_df[\"Price\"], model=\"additive\", period=24 * 4, extrapolate_trend=\"freq\"\n",
    ")\n",
    "result_add_15min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"figure.figsize\": (20, 5)})\n",
    "result_add_60min.plot().suptitle(\"60min - Additive Decompose\")\n",
    "result_add_15min.plot().suptitle(\"15min - Additive Decompose\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multiplicative Decomposition \n",
    "`extrapolate_trend` takes care of any missing values \n",
    "in the trend and residuals at the beginning of the series.\n",
    "\"\"\"\n",
    "print(f\"offset_60min = {abs(expanded_60min_df[\"Price\"].min()) + 0.01}\")\n",
    "print(f\"offset_15min = {abs(expanded_15min_df[\"Price\"].min()) + 0.01}\")\n",
    "\n",
    "# any zero price element?\n",
    "expanded_15min_df[expanded_15min_df[\"Price\"] == 0]\n",
    "# No, therefore, we could also use log transformation.\n",
    "# But lets keep it simple since we only want to understand if there are trends and seasonality for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_60min = abs(expanded_60min_df[\"Price\"].min()) + 0.01  # offseting by the smallest value + 1 cent of EUR\n",
    "expanded_60min_df[\"Price_offset\"] = expanded_60min_df[\"Price\"] + offset_60min\n",
    "expanded_60min_df[[\"Price\", \"Price_offset\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_15min = abs(expanded_15min_df[\"Price\"].min()) + 0.01  # offseting by the smallest value + 1 cent of EUR\n",
    "expanded_15min_df[\"Price_offset\"] = expanded_15min_df[\"Price\"] + offset_15min\n",
    "expanded_15min_df[[\"Price\", \"Price_offset\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double-check if there are negative prices in the offset column\n",
    "print(expanded_60min_df[\"Price_offset\"].min())\n",
    "print(expanded_15min_df[\"Price_offset\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mul_60min = seasonal_decompose(\n",
    "    expanded_60min_df[\"Price_offset\"], model=\"multiplicative\", period=24, extrapolate_trend=\"freq\"\n",
    ")\n",
    "result_mul_15min = seasonal_decompose(\n",
    "    expanded_15min_df[\"Price_offset\"], model=\"multiplicative\", period=96, extrapolate_trend=\"freq\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"figure.figsize\": (20, 5)})\n",
    "result_mul_60min.plot().suptitle(\"60min - Multiplicative Decompose\", fontsize=8)\n",
    "plt.rcParams.update({\"figure.figsize\": (30, 10)})\n",
    "result_mul_15min.plot().suptitle(\"15min - Multiplicative Decompose\", fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations about the time series decomposition:\n",
    "\n",
    "* no obvious trend, with the exception of mid-February to mid-March as already observed\n",
    "* the seasonal component exhibits clear and consistent patterns, which would be even more clear at a higher resolution. This chart suggests a regular daily seasonality in the data, where specific hours of the day are systematically associated with higher or lower prices.\n",
    "* The amplitude (height of the peaks) of the seasonal pattern appears relatively stable, indicating that the magnitude of these daily price fluctuations is consistent over the period analyzed. The amplitude of the seasonal cycles in the 15-minute data is more pronounced, suggesting that the finer granularity captures more intraday price fluctuations.\n",
    "* The residuals are spread out with no clear pattern. There are some spikes on the residual which could generaly be considered outliers or events that the model did not capture (unusual market activities, demand spikes, or errors in data recording). The presence of these spikes in the residuals could indicate a need for models that can handle outliers or for further investigation to ensure data quality.\n",
    "* The 15-minute data captures more detailed fluctuations, which could be important for short-term trading strategies or operational decisions for energy storage and generation.\n",
    "* The larger spikes in the 15-minute residuals may warrant investigation to determine if they are due to actual market events or anomalies in data recording.\n",
    "* While the 60-minute interval data provides a clearer, smoother trend, the 15-minute data captures more detailed variations which might be useful for high-frequency trading algorithms or detailed operational planning.\n",
    "* Both datasets display a similar overall trend and daily seasonality, which indicates these patterns are significant.\n",
    "\n",
    "Let's plot one more visualization to support the observation that no trend is actually there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrended = pd.DataFrame()\n",
    "detrended[\"detrended\"] = expanded_60min_df[\"Price\"] - result_mul_60min.trend\n",
    "detrended[\"Price\"] = expanded_60min_df[\"Price\"]\n",
    "detrended[\"detrended_lsf\"] = signal.detrend(expanded_60min_df[\"Price\"])\n",
    "detrended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "detrended[\"Price\"].plot(label=\"Original\")\n",
    "detrended[\"detrended\"].plot(label='Subtracted the \"least squares fit\"')\n",
    "detrended[\"detrended_lsf\"].plot(label='Subtracted the \"trend component\"')\n",
    "ax.legend()\n",
    "plt.title(\"Detrended series\", fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the trend, you're left with the seasonal and irregular components. Any patterns or cyclical behaviors that remain are due to seasonal effects or other non-trend related factors.\n",
    "The residuals (the detrended prices) fluctuate around zero without a clear pattern or direction, this again suggests a non-pronounced trend component in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deseasonalized = pd.DataFrame()\n",
    "deseasonalized[\"deseasonalized\"] = expanded_60min_df[\"Price\"] / result_mul_60min.seasonal\n",
    "deseasonalized[\"Price\"] = expanded_60min_df[\"Price\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "deseasonalized[\"Price\"].plot(label=\"Original\")\n",
    "deseasonalized[\"deseasonalized\"].plot(label=\"Seasonality removed\")\n",
    "ax.legend()\n",
    "\n",
    "plt.title(\"Deseasonalized prices\", fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationary VS non-stationary\n",
    "\n",
    "Since I'm planing on training different models for forecasting, especially ARIMA (AutoRegressive Integrated Moving Average) or its variations, checking for stationarity is a crucial. On fact, most statistical forecasting methods are based on the assumption that the time series are stationary.\n",
    "\n",
    "In this domain (or finance data in general), one common problem of time series is they are not stationary, which means their statistical properties (mean, variance, maximal and minimal values) change over time. This can be verified with augmented [Dickey-Fuller test](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test). \n",
    "\n",
    "Since several forecasting methods require a stationary series, we need to transform the series somehow to make it stationary. For example:\n",
    "\n",
    "- `difference` the series at least until it become stationary. \n",
    "- take the log of the series if it has non-constant variance.\n",
    "- take the `n-th` root of the series.\n",
    "- fit some type of curve to the data and then model the residuals from that fit, if the data contain a trend.\n",
    "- combination of the above methods\n",
    "\n",
    "\n",
    "To verify if a series is stationary you can use `unit root` tests, like:\n",
    "- visual inspection\n",
    "- rolling statistics\n",
    "- statiscal test (Augmented Dickey-Fuller Test (ADF), Kwiatkowski-Phillips-Schmidt-Shin (KPSS) Test, or Phillips-Perron Test (PP))\n",
    "- Autocorrelation Function (ACF) and Partial Autocorrelation Function PACF plots, to check the correlation between a data point and its lagged values, which allows us to understand how past values influence future values\n",
    "\n",
    "\n",
    "I will use the last option here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_result = adfuller(expanded_60min_df[\"Price\"].values)\n",
    "print(f\"ADF Statistic: {adf_result[0]:.2f}\")\n",
    "print(f\"p-value: {adf_result[1]:.5f}\")\n",
    "print(\"Critical Values:\")\n",
    "for key, value in adf_result[4].items():\n",
    "    print(f\"\\t{key} {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_result = adfuller(expanded_15min_df[\"Price\"].values)\n",
    "print(f\"ADF Statistic: {adf_result[0]:.2f}\")\n",
    "print(f\"p-value: {adf_result[1]:.5f}\")\n",
    "print(\"Critical Values:\")\n",
    "for key, value in adf_result[4].items():\n",
    "    print(f\"\\t{key} {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to make the data stationary\n",
    "# this is the current dataset\n",
    "\n",
    "print(expanded_60min_df[expanded_60min_df[\"Price\"] == 0])  # <--- we still have some zero values to keep in mind\n",
    "\n",
    "# In order to be able to apply log transformation, I need to either remove zeros or transform them.\n",
    "# In the context of energy prices, a zero might indicate a lack of trading or other market anomalies\n",
    "# rather than an actual price of zero. Therefore, one needs to be careful with this transformations.\n",
    "# I will apply a small offset of 0.01 in the price column for the log transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_15min_df\n",
    "print(expanded_15min_df[expanded_15min_df[\"Price\"] == 0])  # <--- No zero price in this dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try several alternative, since we are exploring the data ... it does not cost us anything ;)\n",
    "offset_60min = abs(expanded_60min_df[\"Price\"].min()) + 0.01  # offseting by the smallest value + 1 cent of EUR\n",
    "offset_15min = abs(expanded_15min_df[\"Price\"].min()) + 0.01  # offseting by the smallest value + 1 cent of EUR\n",
    "\n",
    "# differencing\n",
    "expanded_60min_df[\"Price_diff\"] = expanded_60min_df[\"Price\"].diff().dropna()\n",
    "expanded_15min_df[\"Price_diff\"] = expanded_15min_df[\"Price\"].diff().dropna()\n",
    "\n",
    "# # log-transformed price, since we have no zeros in the Price column\n",
    "expanded_60min_df[\"Price_log\"] = np.log(\n",
    "    expanded_60min_df[\"Price\"] + offset_60min\n",
    ")  # handling zeros: use a small offset constant\n",
    "expanded_15min_df[\"Price_log\"] = np.log(expanded_15min_df[\"Price\"] + offset_15min)\n",
    "\n",
    "# # linear detrending\n",
    "expanded_60min_df[\"Price_detrended\"] = signal.detrend(expanded_60min_df[\"Price\"])\n",
    "expanded_15min_df[\"Price_detrended\"] = signal.detrend(expanded_15min_df[\"Price\"])\n",
    "\n",
    "# # Seasonal differencing\n",
    "expanded_60min_df[\"Price_seasonal_diff\"] = expanded_60min_df[\"Price\"].diff(\n",
    "    24\n",
    ")  # assuming 24 observations per day for daily seasonality\n",
    "expanded_15min_df[\"Price_seasonal_diff\"] = expanded_15min_df[\"Price\"].diff(\n",
    "    96\n",
    ")  # assuming 4*24 observations per day for daily seasonality\n",
    "\n",
    "# # Combining Transformations:\n",
    "expanded_60min_df[\"Price_log_diff\"] = np.log(expanded_60min_df[\"Price\"] + offset_60min).diff().dropna()\n",
    "expanded_15min_df[\"Price_log_diff\"] = np.log(expanded_15min_df[\"Price\"] + offset_15min).diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acf_pacf(series: pd.Series, lags: int, title: str = \"\"):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.subplot(211)  # 2 rows, 1 column, 1st subplot\n",
    "    plot_acf(series, ax=plt.gca(), lags=lags)  # Change lags to adjust the number of lags you want to see.\n",
    "    plt.title(f\"{title} - Autocorrelation Function\")\n",
    "\n",
    "    # Plot the Partial Autocorrelation Function (PACF)\n",
    "    plt.subplot(212)  # 2 rows, 1 column, 2nd subplot\n",
    "    plot_pacf(series, ax=plt.gca(), lags=lags)  # Change lags to adjust the number of lags you want to see.\n",
    "    plt.title(f\"{title} - Partial Autocorrelation Function\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_15min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The price related columns are: `Price`, `Price_offset`, `Price_diff`, `Price_log`, `Price_detrended`, `Price_seasonal_diff`, and `Price_log_diff`.\n",
    "\n",
    "Remember `Price` column is not stationary.\n",
    "\n",
    "* `Price_diff`: This is the first-difference of the original series. It typically works well for removing overall trends, making it good for identifying the MA component (q) of an ARIMA model.\n",
    "* `Price_log_diff`: This is the first-difference of the log-transformed series. The log transformation can help stabilize the variance across the series, making this good for financial time series data, which often exhibits exponential growth. Not necessarily fit to our small dataset since no exponential growth is present in this sample.\n",
    "* `Price_detrended`: This series has had a trend component removed, which is useful for highlighting the cyclical nature that might be obscured by the trend.\n",
    "* `Price_seasonal_diff`: This is the seasonal difference of the original series. If there is strong seasonality, this can help to isolate the seasonal patterns for more granular analysis. However, the first cycles in the transformed data will be null due to the nature of the diff transformation. Thus, we will have to remove them or imput the values.\n",
    "* `Price_log`: If you've taken the log of the series to stabilize the variance, this series will help you see relationships that are multiplicative in the original scale.\n",
    "\n",
    "\n",
    "For seasonality, `Price_seasonal_diff` might be revealing, while `Price_diff` or `Price_log_diff` are useful if we are to traing a ARIMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "autocorrelation_plot(expanded_60min_df[\"Price\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrelation_plot(expanded_15min_df[\"Price\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the lags parameter to match the periodicity of the dataset\n",
    "plot_acf_pacf(expanded_60min_df[\"Price_diff\"][1:], lags=24, title=\"60min | Price_diff \")\n",
    "plot_acf_pacf(expanded_60min_df[\"Price_log_diff\"][1:], lags=24, title=\"60min | Price_log_diff \")\n",
    "plot_acf_pacf(expanded_60min_df[\"Price_seasonal_diff\"].dropna(), lags=24, title=\"60min | Price_seasonal_diff \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* the presence of seasonality would yield high autocorrelation.\n",
    "* most data points fall outside the shaded area indicating autocorrelation with a strong statistical significance.\n",
    "* slow decay in the ACF plot indicates that the series has a long memory. Sharp drops might indicate a seasonal pattern.\n",
    "* A sharp cut-off after a few lags in the PACF plot suggests an AR (Autoregressive) process. Again, periodic spikes might point to seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf_pacf(expanded_15min_df[\"Price_diff\"][1:], lags=96, title=\"60min | Price_diff \")\n",
    "plot_acf_pacf(expanded_15min_df[\"Price_log_diff\"][1:], lags=96, title=\"60min | Price_log_diff \")\n",
    "plot_acf_pacf(expanded_15min_df[\"Price_seasonal_diff\"].dropna(), lags=96, title=\"60min | Price_seasonal_diff \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_60min_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
